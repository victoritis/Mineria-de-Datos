{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH7LS4vMFFKi"
      },
      "source": [
        "## Descripción de la práctica\n",
        "\n",
        "En esta práctica se va a utilizar Sklearn para entrenar y probar modelos de clasificación y regresión.\n",
        "\n",
        "En el caso de clasificación se van a usar:\n",
        "- Los datos de la práctica 1 de KNIME, el conjunto completo.\n",
        "- Heart Failure.\n",
        "- Wisconsin Breast Cancer\n",
        "\n",
        "En el caso de regresión se van a utilizar los conjuntos de datos:\n",
        "- Diabetes\n",
        "- Insurance\n",
        "- Life expectancy WHO.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af5Sr7rOFFKj"
      },
      "source": [
        "## Descripción de los datos\n",
        "\n",
        "### Datos de clasificación\n",
        "\n",
        "La descripción de los datos se puede consultar en los siguientes lugares.\n",
        "\n",
        "- Heart Failure.  https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records\n",
        "- Wisconsin Breast Cancer https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic\n",
        "- Diabetes https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html\n",
        "- Insurance https://www.kaggle.com/datasets/mirichoi0218/insurance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbmWulUxFFKf"
      },
      "source": [
        "<img style=\"float:left\" width=\"70%\" src=\"pics/escudo_COLOR_1L_DCHA.png\">\n",
        "<img style=\"float:right\" width=\"15%\" src=\"pics/PythonLogo.svg\">\n",
        "<br style=\"clear:both;\">\n",
        "\n",
        "# Minería de datos\n",
        "\n",
        "<h2 style=\"display: inline-block; padding: 4mm; padding-left: 2em; background-color: navy; line-height: 1.3em; color: white; border-radius: 10px;\">Práctica Scikit-Learn 1</h2>\n",
        "\n",
        "## Docentes\n",
        "\n",
        " - José Francisco Diez Pastor\n",
        "\n",
        "## Estudiantes (1-2)\n",
        "\n",
        "- Víctor Gonzáelez del Campo\n",
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6xs0EocFFKj"
      },
      "source": [
        "<a id=\"index\"></a>\n",
        "## Tareas\n",
        "\n",
        "1. [Carga de los datos. **(3 Puntos)**](#1)\n",
        "2. [Crea una función que evalua conjuntos desbalanceados.**(1 Puntos)**](#2)\n",
        "3. [Evaluar clasificadores con funciones de evaluación propias **(4 Puntos)**](#3)\n",
        "4. [Experimentos con regresores. **(2 Puntos)**](#4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WawCLuDFFFKj"
      },
      "source": [
        "###  Tarea 1. Carga de los datos (3 Puntos)<a id=\"1\"></a><a href=\"#index\"><i class=\"fa fa-list-alt\" aria-hidden=\"true\"></i></a>\n",
        "\n",
        "\n",
        "Realizar 4 funciones, con su documentación (la documentación debe describir que hace la función, los argumentos de entrada y los valores de retorno).\n",
        "- `load_insurance()`.\n",
        "- `load_brest_cancer()`.\n",
        "- `load_heart_failure()`.\n",
        "- `load_mortality()`.\n",
        "\n",
        "Estas funciones se van a crear a imagen y semajanza de las funciones que sirven para cargar los datos de ejemplo en `sklearn`.\n",
        "\n",
        "En `sklearn` las funciones de carga de datos devuelven un diccionario con una clave `data` asociada a los atributos independientes y una clave `target`asociada a la variable dependiente o clase.\n",
        "\n",
        "```Python\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "diabetes_dataset = load_diabetes()\n",
        "X = diabetes_dataset[\"data\"]\n",
        "y = diabetes_dataset[\"target\"]\n",
        "\n",
        "print(X[:5])\n",
        "print(y[:5])\n",
        "```\n",
        "\n",
        "```\n",
        "[[ 0.03807591  0.05068012  0.06169621  0.02187239 -0.0442235  -0.03482076 -0.04340085 -0.00259226  0.01990749 -0.01764613]\n",
        " [-0.00188202 -0.04464164 -0.05147406 -0.02632753 -0.00844872 -0.01916334  0.07441156 -0.03949338 -0.06833155 -0.09220405]\n",
        " [ 0.08529891  0.05068012  0.04445121 -0.00567042 -0.04559945 -0.03419447 -0.03235593 -0.00259226  0.00286131 -0.02593034]\n",
        " [-0.08906294 -0.04464164 -0.01159501 -0.03665608  0.01219057  0.02499059 -0.03603757  0.03430886  0.02268774 -0.00936191]\n",
        " [ 0.00538306 -0.04464164 -0.03638469  0.02187239  0.00393485  0.01559614  0.00814208 -0.00259226 -0.03198764 -0.04664087]]\n",
        "[151.  75. 141. 206. 135.]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0lz6mjfFFKj"
      },
      "source": [
        "Se quiere que las 4 funciones solicitadas trabajen de una manera análoga.\n",
        "\n",
        "Consideraciones:\n",
        "- Algunos datasets tienen atributos nominales y se quieren transformar a binarios utilizando la técnica de one hot encoding. En Python es muy fácil usando el método `get_dummies()` de Pandas.\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "- En los datasets binarios (mortality, breast cancer y heart failure) queremos que los dos posibles valores de las clases sean `t`y `f`. La clase positiva (`t`) va a ser siempre la minoritaria.\n",
        "- Las 4 funciones son muy similares, así que puedes pensar como implementarlas para evitar código duplicado.\n",
        "\n",
        "Pasos:\n",
        "1. Carga los datos, utiliza Pandas para ello, por ejemplo `read_csv`.\n",
        "2. Separa los atributos del valor a predecir o clase.\n",
        "3. Convierte los atributos nominales a binarios con get_dummies.\n",
        "4. En el caso de los conjuntos de clasificación fuerza que las clases sean `t`y `f`y `t`sea la clase minoritaria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO8oRuz5FFKj"
      },
      "source": [
        "#### Resultado esperado\n",
        "![imagen.png](attachment:imagen.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "PEoEpgz3FFKk",
        "outputId": "c4cc0980-6c1c-4a7b-e5ec-cbcf14ccb52d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/insurance.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-20af4e0b89ef>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m datasets = [load_diabetes(),load_insurance(),\n\u001b[0m\u001b[1;32m     94\u001b[0m             load_cancer(), load_heart_failure(),load_survival()]\n\u001b[1;32m     95\u001b[0m nombres = [\"Diabetes\", \"Insurance\",\n",
            "\u001b[0;32m<ipython-input-1-20af4e0b89ef>\u001b[0m in \u001b[0;36mload_insurance\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Carga los datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/insurance.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# eliminando la clase (target) nos quedamos solo con los datos de los atributos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/insurance.csv'"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import load_diabetes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_dataset(train_data,taget_data):\n",
        "\n",
        "    # Convierte los atributos nominales a binarios con get_dummies\n",
        "    X = pd.get_dummies(tarin_data).astype(float)\n",
        "\n",
        "    return {'data': X.values, 'target': y.values}\n",
        "\n",
        "\n",
        "def load_insurance():\n",
        "\n",
        "    # Carga los datos\n",
        "    df = pd.read_csv('/content/insurance.csv')\n",
        "\n",
        "    # eliminando la clase (target) nos quedamos solo con los datos de los atributos\n",
        "    train_data = df.drop([\"charges\"], axis=1)\n",
        "\n",
        "    # Selecciono target (y) (que en esta dataset se llama Class)\n",
        "    target_data = df[\"charges\"].values\n",
        "\n",
        "    # Cuenta el número de valores únicos en target_data\n",
        "    num_unique_values = len(np.unique(target_data))\n",
        "\n",
        "    # Si el número de valores únicos es igual a 2\n",
        "    is_classification = num_unique_values == 2\n",
        "\n",
        "    return load_dataset(train_data,target_data, is_classification)\n",
        "\n",
        "def load_cancer():\n",
        "\n",
        "    # Carga los datos\n",
        "    df = pd.read_csv('/content/wisconsin.csv')\n",
        "\n",
        "    # eliminando la clase (target) nos quedamos solo con los datos de los atributos\n",
        "    train_data = df.drop([\"diagnosis\"], axis=1)\n",
        "\n",
        "    # Selecciono target (y) (que en esta dataset se llama Class)\n",
        "    target_data = df[\"diagnosis\"].values\n",
        "\n",
        "    # Cuenta el número de valores únicos en target_data\n",
        "    num_unique_values = len(np.unique(target_data))\n",
        "\n",
        "    # Si el número de valores únicos es igual a 2\n",
        "    is_classification = num_unique_values == 2\n",
        "\n",
        "    return load_dataset(train_data,target_data,is_classification)\n",
        "\n",
        "def load_heart_failure():\n",
        "\n",
        "    # Carga los datos\n",
        "    df = pd.read_csv('/content/heart_failure_clinical_records_dataset.csv')\n",
        "\n",
        "    # eliminando la clase (target) nos quedamos solo con los datos de los atributos\n",
        "    train_data = df.drop([\"DEATH_EVENT\"], axis=1)\n",
        "\n",
        "    # Selecciono target (y) (que en esta dataset se llama Class)\n",
        "    target_data = df[\"DEATH_EVENT\"].values\n",
        "\n",
        "    # Cuenta el número de valores únicos en target_data\n",
        "    num_unique_values = len(np.unique(target_data))\n",
        "\n",
        "    # Si el número de valores únicos es igual a 2\n",
        "    is_classification = num_unique_values == 2\n",
        "\n",
        "    return load_dataset(train_data,target_data,is_classification)\n",
        "\n",
        "def load_survival():\n",
        "\n",
        "    # Carga los datos\n",
        "    df = pd.read_csv('/content/all.csv')\n",
        "\n",
        "    # eliminando la clase (target) nos quedamos solo con los datos de los atributos\n",
        "    train_data = df.drop([\"Class\"], axis=1)\n",
        "\n",
        "    # Selecciono target (y) (que en esta dataset se llama Class)\n",
        "    target_data = df[\"Class\"].values\n",
        "\n",
        "    # Cuenta el número de valores únicos en target_data\n",
        "    num_unique_values = len(np.unique(target_data))\n",
        "\n",
        "    # Si el número de valores únicos es igual a 2\n",
        "    is_classification = num_unique_values == 2\n",
        "\n",
        "    return load_dataset(train_data,target_data,is_classification)\n",
        "\n",
        "\n",
        "\n",
        "datasets = [load_diabetes(),load_insurance(),\n",
        "            load_cancer(), load_heart_failure(),load_survival()]\n",
        "nombres = [\"Diabetes\", \"Insurance\",\n",
        "           \"Cancer\",\"Heart Failure\",\"Survival\"]\n",
        "\n",
        "for dataset,nombre in zip(datasets,nombres):\n",
        "\n",
        "    X = dataset[\"data\"]\n",
        "    y = dataset[\"target\"]\n",
        "    print(f\"________{nombre}________\")\n",
        "    print(\"Primera fila\",X[0])\n",
        "    print(\"Medias X\",X.mean(axis=0))\n",
        "    print(\"Primeras ys\",y[:10])\n",
        "    if nombre in [\"Diabetes\", \"Insurance\"]:\n",
        "        print(\"Media y\",y.mean())\n",
        "    else:\n",
        "        print(\"Frecuencias y\",np.unique(y, return_counts=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBpMXK2JFFKl"
      },
      "source": [
        "### Tarea 2.Crea una función que evalua conjuntos desbalanceados (1 Puntos)<a id=\"2\"></a><a href=\"#index\"><i class=\"fa fa-list-alt\" aria-hidden=\"true\"></i></a>\n",
        "\n",
        "En un conjunto desbalanceado, en el que el número de ejemplos de una clase es muy superior al número de ejemplos de la otra, no es tan útil utilizar el `accuracy_score` (tasa de acierto). Por ejemplo podría darse el caso de un conjunto de datos en el que tuviesemos un 99% de ejemplos pertenecientes a pacientes negativos en una determinada enfermedad y un 1% en positivos. Un clasificador poco inteligente que siempre predijese la clase mayoritaria acertaría el 99% de las veces pero no sería nada útil.\n",
        "\n",
        "Para esos casos se utilizan medidas derivadas de la matriz de confusión.\n",
        "\n",
        "|                 | Positiva Pred | Negativa Pred   |\n",
        "|-----------------|---------------|-----------------|\n",
        "| Positiva Real   |      TP       |      FN         |\n",
        "| Negativa Real   |      FP       |      TN         |\n",
        "\n",
        "- precision = TP / (TP + FP)\n",
        "- recall = TP / (TP + FN)\n",
        "\n",
        "- f1 = 2 (precision x recall) / (precision + recall)\n",
        "\n",
        "- TNR = TN / (TN+FN)\n",
        "- TPR = TP / (TP+FP)\n",
        "\n",
        "- balanced accuracy = (TNR + TPR)/2\n",
        "\n",
        "\n",
        "Como los conjuntos de datos de clasificación que se van a usar en esta práctica son desbalanceados, se pide implementar una función para la tasa de acierto balanceada\n",
        "\n",
        "Realizar y documentar una función:\n",
        "- `my_balanced_accuracy`. Recibe `y_true` los valores de las clases correctos, `y_pred` las predicciones. Devuelve la tasa de acierto balanceada.\n",
        "\n",
        "Se puede comparar su buen funcionamiento comparandola con la oficial de `Sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUh6ppeUFFKm"
      },
      "outputs": [],
      "source": [
        "def my_balanced_accuracy(y_true, y_pred):\n",
        "    # completa el código\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VwrlGUYFFKm",
        "outputId": "9ad16787-c694-4fd5-82a3-028e30ffead7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.625\n",
            "0.625\n",
            "0.7083333333333333\n",
            "0.7083333333333333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = [0, 1, 0, 0, 1, 0]\n",
        "y_pred = [0, 1, 0, 0, 0, 1]\n",
        "\n",
        "\n",
        "\n",
        "print(balanced_accuracy_score(y_true, y_pred))\n",
        "print(my_balanced_accuracy(y_true, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "y_true = [\"F\", \"T\", \"F\", \"F\", \"T\", \"F\",\"T\"]\n",
        "y_pred = [\"F\", \"T\", \"F\", \"F\", \"F\", \"T\",\"T\"]\n",
        "\n",
        "print(balanced_accuracy_score(y_true, y_pred))\n",
        "print(my_balanced_accuracy(y_true, y_pred))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8au4aFRFFKm"
      },
      "source": [
        "### Tarea 3. Evaluar clasificadores con funciones de evaluación propias. (4 Puntos)<a id=\"3\"></a><a href=\"#index\"><i class=\"fa fa-list-alt\" aria-hidden=\"true\"></i></a>\n",
        "\n",
        "Crear y documenta la función:\n",
        "\n",
        "- `evalua(dataset, method, folds, metric)` La métrica indicada para cada una de las folds indicadas. Utilizando el dataset (diccionario con clave data y clave tarjet) y el método de clasificación indicado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXgt7lp7FFKm"
      },
      "source": [
        "```Python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "dataset = load_cancer()\n",
        "\n",
        "acc_scorer = make_scorer(accuracy_score)\n",
        "bal_acc_scorer = make_scorer(my_balanced_accuracy)\n",
        "\n",
        "\n",
        "evalua(dataset,KNeighborsClassifier(),10,bal_acc_scorer)\n",
        "```\n",
        "\n",
        "Devuelve\n",
        "\n",
        "```\n",
        "array([0.90324675, 0.84935065, 0.88690476, 0.95238095, 0.93849206,\n",
        "       0.92460317, 0.96230159, 0.91468254, 0.92063492, 0.96190476])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW26dw9sFFKn"
      },
      "outputs": [],
      "source": [
        "def evalua(dataset, method, folds, metric):\n",
        "    # completa el código\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzwJFzbGFFKn",
        "outputId": "d9c076b2-93eb-414a-f421-e307b0d584d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.91071429, 0.89558824, 0.88690476, 0.97368421, 0.94797297,\n",
              "       0.92460317, 0.96230159, 0.93421053, 0.90151515, 0.96190476])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "dataset = load_cancer()\n",
        "\n",
        "acc_scorer = make_scorer(accuracy_score)\n",
        "bal_acc_scorer = make_scorer(my_balanced_accuracy)\n",
        "\n",
        "\n",
        "evalua(dataset,KNeighborsClassifier(),10,bal_acc_scorer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQSzyz9aFFKn"
      },
      "source": [
        "Haz una una tabla de resultados para cada una de las 3 medidas: tasa de acierto, tasa de acierto balanceada, F1.\n",
        "\n",
        "![imagen.png](attachment:imagen.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUPOiGJYFFKo"
      },
      "source": [
        "#### Comentarios\n",
        "\n",
        "1. Trata de hacer la tabla automáticamente utilizando DataFrames de Pandas.\n",
        "2. Prueba varios clasificadores por lo menos los que se ven en la tabla: Vecinos más cercanos y árbol de decisión.\n",
        "2. Los ensembles son conjuntos de clasificadores. Los ensembles pueden a menudo obtener mejores resultados que los clasificadores base de los que están formados (Investiga que ensembles hay en Sklearn y pruebalos).\n",
        "    - Las ganancias obtenidas al añadir clasificadores a un ensemble son cada vez menores a medida que el ensemble crece.\n",
        "3. Hay muchas estrategías para lidiar con conjuntos de datos desequilibrados. Una es reducir el tamaño de la clase mayoritaria eliminando ejemplos de forma aleatoria (Random Undersampling) y otra es aumentar el tamaño de la clase minoritaria creando ejemplos artificiales (SMOTE, Oversampling)\n",
        "    - Hay ensembles que tienen incorporadas estas estrategias (https://imbalanced-learn.org/stable/references/ensemble.html), puedes intentar probarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD-8R6TIFFKo",
        "outputId": "69b5773b-119f-4e02-c90a-2f7d98a43ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Method</th>\n",
              "      <th>KNN</th>\n",
              "      <th>TREE</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Data</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Cancer</th>\n",
              "      <td>0.929762</td>\n",
              "      <td>0.905138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Heart</th>\n",
              "      <td>0.641954</td>\n",
              "      <td>0.678736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Survival</th>\n",
              "      <td>0.809324</td>\n",
              "      <td>0.892684</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Method         KNN      TREE\n",
              "Data                        \n",
              "Cancer    0.929762  0.905138\n",
              "Heart     0.641954  0.678736\n",
              "Survival  0.809324  0.892684"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Haz una tabla para cada medida\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxRfkdvnFFKp"
      },
      "source": [
        "### Tarea 4. Experimentos regresores. (2 Puntos)<a id=\"4\"></a><a href=\"#index\"><i class=\"fa fa-list-alt\" aria-hidden=\"true\"></i></a>\n",
        "\n",
        "Realiza un experimento similar, pero esta vez utilizando los dos conjuntos de datos de regresión.\n",
        "\n",
        "Utiliza al menos Vecinos más cercanos y árbol de decisión.\n",
        "\n",
        "Investiga los Ensembles existentes en Sklearn para regresión.\n",
        "\n",
        "A la hora de evaluar los regresores puedes usar R2 (sklearn.metrics.r2_score), también llamado coeficiente de determinación.\n",
        "\n",
        "En esa métrica el mejor valor posible es 1.0 y puede obtener valores negativos. Un regresor que siempre devuelve la media, sin tener en cuenta los valores de los atributos obtendría un R2 = 0.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkOgpbfiFFKp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MibP4GlhFFKp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}